{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1277eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\david\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\david\\anaconda3\\lib\\site-packages (1.9.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\david\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\david\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\david\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\david\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\david\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import string, re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac928cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da31efdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': {'howdy', 'how do you do', 'hullo', 'hello', 'hi'}, 'bye': {'cheerio', 'so long', 'good by', 'sayonara', 'adieu', 'auf wiedersehen', 'au revoir', 'bye bye', 'arrivederci', 'bye', 'good day', 'goodbye', 'adios', 'goodby', 'pass', 'good bye'}, 'elaborate': {'expand', 'complicate', 'work out', 'luxuriant', 'detailed', 'lucubrate', 'exposit', 'expatiate', 'flesh out', 'refine', 'expound', 'enlarge', 'rarify', 'elaborated', 'elaborate', 'dilate'}}\n"
     ]
    }
   ],
   "source": [
    "# building keywords, hello and bye with their synyonyms\n",
    "list_words=['hello','bye','elaborate']\n",
    "list_syn={}\n",
    "for word in list_words:\n",
    "    synonyms=[]\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lem in syn.lemmas():\n",
    "            # Remove any special characters from synonym strings\n",
    "            lem_name = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', lem.name())\n",
    "            synonyms.append(lem_name)\n",
    "    list_syn[word]=set(synonyms)\n",
    "print (list_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06960283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greet': re.compile('.*\\\\bhowdy\\\\b.*|.*\\\\bhow do you do\\\\b.*|.*\\\\bhullo\\\\b.*|.*\\\\bhello\\\\b.*|.*\\\\bhi\\\\b.*'), 'bye': re.compile('.*\\\\bcheerio\\\\b.*|.*\\\\bso long\\\\b.*|.*\\\\bgood by\\\\b.*|.*\\\\bsayonara\\\\b.*|.*\\\\badieu\\\\b.*|.*\\\\bauf wiedersehen\\\\b.*|.*\\\\bau revoir\\\\b.*|.*\\\\bbye bye\\\\b.*|.*\\\\barrivederci\\\\b.*|.*\\\\bbye\\\\b.*|.*\\\\bgood ), 'elaborate': re.compile('.*\\\\bexpand\\\\b.*|.*\\\\bcomplicate\\\\b.*|.*\\\\bwork out\\\\b.*|.*\\\\bluxuriant\\\\b.*|.*\\\\bdetailed\\\\b.*|.*\\\\blucubrate\\\\b.*|.*\\\\bexposit\\\\b.*|.*\\\\bexpatiate\\\\b.*|.*\\\\bflesh out\\\\b.*|.*\\\\brefine\\\\b.*|.*\\\\bexp)}\n"
     ]
    }
   ],
   "source": [
    "# Building dictionary of Intents & Keywords\n",
    "keywords={}\n",
    "keywords_dict={}\n",
    "# Defining a new key in the keywords dictionary\n",
    "keywords['greet']=[]\n",
    "# Populating the values in the keywords dictionary with synonyms of keywords formatted with RegEx metacharacters \n",
    "for synonym in list(list_syn['hello']):\n",
    "    keywords['greet'].append('.*\\\\b'+synonym+'\\\\b.*')\n",
    "\n",
    "# Defining a new key in the keywords dictionary\n",
    "keywords['bye']=[]\n",
    "# Populating the values in the keywords dictionary with synonyms of keywords formatted with RegEx metacharacters \n",
    "for synonym in list(list_syn['bye']):\n",
    "    keywords['bye'].append('.*\\\\b'+synonym+'\\\\b.*')\n",
    "    \n",
    "# Defining a new key in the keywords dictionary\n",
    "keywords['elaborate']=[]\n",
    "for synonym in list(list_syn['elaborate']):\n",
    "    keywords['elaborate'].append('.*\\\\b'+synonym+'\\\\b.*')\n",
    "    \n",
    "for intent, keys in keywords.items():\n",
    "    # Joining the values in the keywords dictionary with the OR (|) operator updating them in keywords_dict dictionary\n",
    "    keywords_dict[intent]=re.compile('|'.join(keys))\n",
    "print (keywords_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09cb418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build responses\n",
    "responses={\n",
    "    'greet':'Hola! How are you?',\n",
    "    'bye':'See Ya Later, Alligator!',\n",
    "    'elaborate':'Huh? Nothing to add here.',\n",
    "    'fallback':'What do you meme?',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b378bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm bot bot, how can I help you?\n",
      "elaborate\n",
      "Huh? Nothing to add here.\n"
     ]
    }
   ],
   "source": [
    "print (\"Hi, I'm bot bot, how can I help you?\")\n",
    "# While loop to run the chatbot indefinetely\n",
    "while (True):  \n",
    "    # Takes the user input and converts all characters to lowercase\n",
    "    user_input = input().lower()\n",
    "    # Defining the Chatbot's exit condition\n",
    "    if user_input == 'quit': \n",
    "        print (\"Byeeee~.\")\n",
    "        break    \n",
    "    matched_intent = None \n",
    "    for intent,pattern in keywords_dict.items():\n",
    "        # Using the regular expression search function to look for keywords in user input\n",
    "        if re.search(pattern, user_input): \n",
    "            # if a keyword matches, select the corresponding intent from the keywords_dict dictionary\n",
    "            matched_intent=intent  \n",
    "    # The fallback intent is selected by default\n",
    "    key='fallback' \n",
    "    if matched_intent in responses:\n",
    "        # If a keyword matches, the fallback intent is replaced by the matched intent as the key for the responses dictionary\n",
    "        key = matched_intent\n",
    "    # The chatbot prints the response that matches the selected intent\n",
    "    print (responses[key]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290047a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
